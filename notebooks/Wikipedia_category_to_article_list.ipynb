{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wikipedia category to article list.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPSA00ddjk82cRGXhDRt9EL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacomyma/mapping-controversies/blob/main/notebooks/Wikipedia_category_to_article_list.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wikipedia category to article list\n",
        "\n",
        "**Input:** the name of a Wikipedia category.\n",
        "\n",
        "**Output:** a list of Wikipedia articles (CSV).\n",
        "\n",
        "This scripts queries Wikipedia to get the list of articles in a Wikipedia category, including its subcategories (and so on recursively).\n",
        "\n",
        "\n",
        "## How to use\n",
        "\n",
        "1. Edit the settings\n",
        "1. Run all the cells\n",
        "1. Take the output file from the notebook folder\n"
      ],
      "metadata": {
        "id": "nxpqCgYwPkhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETTINGS"
      ],
      "metadata": {
        "id": "eNsu45KQQEws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wikipedia category to extract\n",
        "# Note: use the way it is displayed in the corresponding page.\n",
        "#       For instance this page: https://en.wikipedia.org/wiki/Category%3AComputer_ethics\n",
        "#       Gives you this category name: \"Category:Computer ethics\"\n",
        "category_to_extract = \"Category:Computer ethics\"\n",
        "\n",
        "# Output file\n",
        "output_file = \"wikipedia-articles.csv\""
      ],
      "metadata": {
        "id": "n-aJ1ZH7QFoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCRIPT"
      ],
      "metadata": {
        "id": "k8baF6r-Q97q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import libraries\n",
        "This notebook draws on existing code.\n",
        "You can ignore the output."
      ],
      "metadata": {
        "id": "FxXRQUWWRAEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install (if needed)\n",
        "!pip install wikipedia-api\n",
        "!pip install wikipedia\n",
        "!pip install pandas\n",
        "\n",
        "# Import\n",
        "import wikipediaapi\n",
        "import wikipedia\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "rOGkX4ClRBJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Harvest the category"
      ],
      "metadata": {
        "id": "ot2EUVUMRBxF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVjObj4iPQFr"
      },
      "outputs": [],
      "source": [
        "# Create empty set of articles to fill later on\n",
        "article_set = set()\n",
        "\n",
        "# This is an object we use to connect to the API.\n",
        "# Note that we configure it to use the English Wikipedia.\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "  language='en',\n",
        "  extract_format = wikipediaapi.ExtractFormat.WIKI\n",
        ")\n",
        "\n",
        "# Create the category object (stuff specific to the API library)\n",
        "cat = wiki_wiki.page(category_to_extract)\n",
        "\n",
        "# Recursively build the list of pages (because there are sub-categories)\n",
        "# For the recursion, we create a function that might call itself\n",
        "def parse_categorymembers(categorymembers, level=0, max_level=2):\n",
        "  for c in categorymembers.values():\n",
        "    if c.ns == wikipediaapi.Namespace.MAIN: # This element is an article\n",
        "      article_set.add(c.title)\n",
        "    if (c.ns == wikipediaapi.Namespace.CATEGORY and\n",
        "      level < max_level): # This element is a sub-category\n",
        "      parse_categorymembers(c.categorymembers,\n",
        "                            level=level + 1,\n",
        "                            max_level=max_level)\n",
        "parse_categorymembers(cat.categorymembers)\n",
        "\n",
        "# Transform the set into a data frame for convenience\n",
        "article_df = pd.DataFrame(article_set, columns=[\"Article\"])\n",
        "\n",
        "# Output the data frame to check if it works\n",
        "print(\"Preview of the article list:\")\n",
        "article_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save list as CSV"
      ],
      "metadata": {
        "id": "BQv1pbKkSRtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  article_df.to_csv(output_file, index = False, encoding='utf-8')\n",
        "  print(\"Done.\")\n",
        "except IOError:\n",
        "  print(\"/!\\ Error while writing the output file\")"
      ],
      "metadata": {
        "id": "pSDGocD_Q3zt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}