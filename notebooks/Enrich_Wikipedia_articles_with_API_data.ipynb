{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Enrich Wikipedia articles with API data",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIHxUjd0hGQOB9AvKyghX+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacomyma/mapping-controversies/blob/main/notebooks/Enrich_Wikipedia_articles_with_API_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enrich Wikipedia articles with API data\n",
        "\n",
        "**Input:** a list of Wikipedia articles (CSV).\n",
        "\n",
        "**Output:** a list of Wikipedia articles with additional columns (CSV).\n",
        "\n",
        "This scripts queries Wikipedia for each article of the input list. It retrieves additional information for each article. It saves the enriched list.\n",
        "\n",
        "*NOTE: THE SCRIPT IS SLOW.*\n",
        "\n",
        "## How to use\n",
        "\n",
        "1. Put your input file in the same folder as the notebook\n",
        "1. Edit the settings if needed\n",
        "1. Run all the cells\n",
        "1. Take the output file from the notebook folder"
      ],
      "metadata": {
        "id": "430TLRHM3lM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List of available additional information\n",
        "\n"
      ],
      "metadata": {
        "id": "YGLrTkju3_G1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Correct title\n",
        "* URL\n",
        "* Summary (text)\n",
        "* Images (number and/or list)\n",
        "* Links (number and/or list)\n",
        "* Categories (number and/or list)\n",
        "* References (number and/or list)"
      ],
      "metadata": {
        "id": "uLeI1btc4p6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETTINGS"
      ],
      "metadata": {
        "id": "PGZnyNfE45tK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUfRQCfc3bkV"
      },
      "outputs": [],
      "source": [
        "# Input file\n",
        "input_file = \"wikipedia-articles.csv\"\n",
        "\n",
        "# Which column contains the article title?\n",
        "article_name_column = \"Article\"\n",
        "\n",
        "# Output file\n",
        "output_file = \"wikipedia-articles-enriched.csv\"\n",
        "\n",
        "# Which information do you need? Keep your file size in check!\n",
        "get_correct_title = True\n",
        "get_URL = True\n",
        "get_summary = True\n",
        "get_images_count = True\n",
        "get_images_list = False\n",
        "get_links_count = True\n",
        "get_links_list = False\n",
        "get_categories_count = True\n",
        "get_categories_list = False\n",
        "get_references_count = True\n",
        "get_references_list = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCRIPT"
      ],
      "metadata": {
        "id": "zyTRs3ov5fs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import libraries\n",
        "This notebook draws on existing code.\n",
        "You can ignore the output."
      ],
      "metadata": {
        "id": "dxHiS4qg5kwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install (if needed)\n",
        "!pip install wikipedia\n",
        "!pip install pandas\n",
        "\n",
        "# Import\n",
        "import wikipedia\n",
        "import pandas as pd\n",
        "import io\n",
        "import csv\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "2XlbFssu5go6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the input file"
      ],
      "metadata": {
        "id": "1ZszgEBA5rrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article_df = pd.read_csv(input_file, quotechar='\"', encoding='utf8', doublequote=True, quoting=csv.QUOTE_NONNUMERIC, dtype=object)\n",
        "print(\"Preview of the article list:\")\n",
        "article_df"
      ],
      "metadata": {
        "id": "9BMChaOn5tS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Harvest Wikipedia"
      ],
      "metadata": {
        "id": "0smmiEaZ5vn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A CSV to String function\n",
        "def csvize(csvdata):\n",
        "  output = io.StringIO()\n",
        "  writer = csv.writer(output, quoting=csv.QUOTE_MINIMAL)\n",
        "  writer.writerow(csvdata)\n",
        "  return output.getvalue()\n",
        "\n",
        "# Declare the data harvesting function\n",
        "def harvest(title):\n",
        "  try:\n",
        "    page = wikipedia.page(title,auto_suggest=False)\n",
        "\n",
        "  except wikipedia.exceptions.DisambiguationError:\n",
        "    print(\"Wikipedia thinks \"+title+\" is ambiguous (returns several candidate pages). Trying again with all capitalized letters\")\n",
        "    try:\n",
        "      page = wikipedia.page(title.capitalize(),auto_suggest=False)\n",
        "      print(\"Success! \"+title+\" is no longer ambiguous\")\n",
        "    except wikipedia.exceptions.DisambiguationError:\n",
        "      print(\"Wikipedia still thinks \"+title+\" is ambiguous (returns several candidate pages). Trying again with all lower letters\")\n",
        "      try:\n",
        "        page = wikipedia.page(title.lower(),auto_suggest=False)\n",
        "        print(\"Success! \"+title+\" is no longer ambiguous\")\n",
        "      except wikipedia.exceptions.DisambiguationError:\n",
        "        print(\"Wikipedia still thinks \"+title+\" is ambiguous (returns several candidate pages). Skipping page...\")\n",
        "        return []\n",
        "  except wikipedia.exceptions.PageError:\n",
        "    print(\"The page \"+title+\" could not be found. Skipping page...\")\n",
        "    return []\n",
        "\n",
        "  except:\n",
        "    print(\"The page \"+title+\" failed due to unknown reason. Skipping...\")\n",
        "    print(\"\")\n",
        "    return []\n",
        "  \n",
        "  data = {}\n",
        "  if get_correct_title:\n",
        "    data['correct-title'] = page.title\n",
        "  if get_URL:\n",
        "    data['url'] = page.url\n",
        "  if get_summary:\n",
        "    data['summary'] = page.summary\n",
        "  if get_images_count:\n",
        "    data['images-count'] = len(page.images)\n",
        "  if get_images_list:\n",
        "    data['images'] = csvize(page.images)\n",
        "  if get_links_count:\n",
        "    data['links-count'] = len(page.links)\n",
        "  if get_links_list:\n",
        "    data['links'] = csvize(page.links)\n",
        "  if get_categories_count:\n",
        "    data['categories-count'] = len(page.categories)\n",
        "  if get_categories_list:\n",
        "    data['categories'] = csvize(page.categories)\n",
        "  if get_references_count:\n",
        "    data['references-count'] = len(page.references)\n",
        "  if get_references_list:\n",
        "    data['references'] = csvize(page.references)\n",
        "  return data\n",
        "  \n",
        "# Harvest each article one by one\n",
        "enriched_article_list = []\n",
        "seen = []\n",
        "print(\"Harvesting information from \"+str(len(article_df.index))+\" wikipedia pages. This might take a while...\")\n",
        "count=1\n",
        "for index, row in article_df.iterrows():\n",
        "  title = row[article_name_column]\n",
        "  if count % 10 == 0:\n",
        "    print(\"Additional information harvested from \"+str(count)+\" pages out of \"+str(len(article_df.index))+\". Continuing...\")\n",
        "  if not title in seen: # Do not harvest twice the same...\n",
        "    seen.append(title)\n",
        "    try:\n",
        "      data = harvest(title)\n",
        "      new_row = {**row, **data}\n",
        "      enriched_article_list.append(new_row)\n",
        "    except:\n",
        "        print('SKIPPED: '+title+' (an error occurred)')\n",
        "  count=count+1\n",
        "\n",
        "enriched_articles_df = pd.DataFrame(enriched_article_list)\n",
        "print(\"Done.\")\n",
        "print(\"Preview of the enriched article list:\")\n",
        "enriched_articles_df"
      ],
      "metadata": {
        "id": "-IUFIjKZ5wPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save as CSV"
      ],
      "metadata": {
        "id": "NUGRNqP490uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  enriched_articles_df.to_csv(output_file, index = False, encoding='utf-8')\n",
        "  print(\"Done.\")\n",
        "except IOError:\n",
        "  print(\"/!\\ Error while writing the output file\")"
      ],
      "metadata": {
        "id": "GHEUuYVg94H_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}