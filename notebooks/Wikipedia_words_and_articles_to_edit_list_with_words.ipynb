{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wikipedia words and articles to edit list with words",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacomyma/mapping-controversies/blob/main/notebooks/Wikipedia_words_and_articles_to_edit_list_with_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üç± Wikipedia words and articles to edit list with words"
      ],
      "metadata": {
        "id": "JusW97SUGBgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inputs:**\n",
        "* a SMALL list of Wikipedia articles (CSV)\n",
        "* a small list of words, like a dozen (CSV)\n",
        "\n",
        "**Outputs:**\n",
        "* a list of term-revision pairs, with article and timestamp (CSV)\n",
        "\n",
        "This script tells you which words are in which revisions for which article, and when."
      ],
      "metadata": {
        "id": "31PaqnfcGDWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use"
      ],
      "metadata": {
        "id": "cVzHKii6HvCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Put your input files in the same folder as the notebook\n",
        "1. Edit the settings if needed.\n",
        "1. Run all the cells\n",
        "1. Take the output file from the notebook folder"
      ],
      "metadata": {
        "id": "yiVPSZzJHz22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETTINGS"
      ],
      "metadata": {
        "id": "2aMAjhGDH2qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input file 1: Wikipedia articles\n",
        "input_file_articles = \"wikipedia-articles.csv\"\n",
        "# Which column contains the article title?\n",
        "article_name_column = \"Article\"\n",
        "\n",
        "# Input file 2: small list of words\n",
        "input_file_words = \"words-small-list.csv\"\n",
        "# Which column contains the words?\n",
        "words_text_column = \"text\"\n",
        "\n",
        "# Output files\n",
        "output_file = \"terms-and-revisions.csv\""
      ],
      "metadata": {
        "id": "ODgqeaStH3kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCRIPT"
      ],
      "metadata": {
        "id": "9d_s_6zbH40x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import libraries\n",
        "This notebook draws on existing code.\n",
        "You can ignore the output."
      ],
      "metadata": {
        "id": "Swaxx6G4H8yS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU-bf9BtvSJN"
      },
      "source": [
        "# Install (if needed)\n",
        "!pip install pandas\n",
        "!pip install requests\n",
        "\n",
        "# Import\n",
        "import csv\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the input file 1 (documents)"
      ],
      "metadata": {
        "id": "d0FSMimJIZtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article_df = pd.read_csv(input_file_articles, quotechar='\"', encoding='utf8', doublequote=True, quoting=csv.QUOTE_NONNUMERIC, dtype=object)\n",
        "print(\"Preview of the article list:\")\n",
        "article_df"
      ],
      "metadata": {
        "id": "3OF7BDMiIbok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the input file 2 (words)"
      ],
      "metadata": {
        "id": "U9uSFdYiKoAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_df = pd.read_csv(input_file_words, quotechar='\"', encoding='utf8', doublequote=True, quoting=csv.QUOTE_NONNUMERIC, dtype=object)\n",
        "print(\"Preview of the word list:\")\n",
        "word_df"
      ],
      "metadata": {
        "id": "FXNwsPgVKlWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Harvest Wikipedia"
      ],
      "metadata": {
        "id": "eczPM9TFMO3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Index the terms\n",
        "terms = set()\n",
        "for index, row in word_df.iterrows():\n",
        "  terms.add(row[words_text_column])\n",
        "\n",
        "# Make a dump for security\n",
        "dump_filename = \"dump-data.csv\"\n",
        "\n",
        "# Define an empty dataframe for the output datafile\n",
        "df = pd.DataFrame(columns=['Page','OldRevision_Url','Time','Term'])\n",
        "\n",
        "# Iterate over the list of pages\n",
        "for title in article_df[article_name_column]:\n",
        "  URL = \"http://en.wikipedia.org/w/api.php\" # we are going to call the API for English Wikipedia\n",
        "  S = requests.Session()\n",
        "    \n",
        "  # Below some paramters for the API query. We are getting the ID and timestamp for each revision.\n",
        "  PARAMS = {\n",
        "    \"action\": \"query\",\n",
        "    \"prop\": \"revisions\",\n",
        "    \"titles\": title,\n",
        "    \"rvlimit\": \"500\",\n",
        "    \"rvprop\": \"timestamp|ids|content\",\n",
        "    \"rvdir\": \"newer\",\n",
        "    \"rvstart\": \"2001-01-01\"+\"T00:00:00Z\",\n",
        "    \"formatversion\": \"2\",\n",
        "    \"format\": \"json\"\n",
        "  }\n",
        "\n",
        "  R = S.get(url=URL, params=PARAMS)\n",
        "  if R.status_code==404:\n",
        "    print(\"The page does not exist\")\n",
        "  DATA = R.json()\n",
        "  for each in DATA['query']['pages']:\n",
        "    for revision in each['revisions']:\n",
        "      for term in terms:\n",
        "        if 'content' in revision.keys():\n",
        "          row = [title,'https://en.wikipedia.org/w/index.php?title='+title+'&oldid='+str(revision['revid']),revision['timestamp']]\n",
        "          # Search for the term\n",
        "          if term.lower() in revision['content'].lower():\n",
        "            # and add a result to the data output if the term is found\n",
        "            row.append(term)\n",
        "            df.loc[len(df)] = row\n",
        "\n",
        "    # Dump the latest version of the reuslts\n",
        "    df.to_csv(dump_filename)\n",
        "    print('Queried another 500 revisions for ' + title + ' until '+revision['timestamp'])\n",
        "  \n",
        "  # When there are more than 500 revisions we need this addition to keep paging through the revisions.\n",
        "  while 'continue' in DATA.keys():\n",
        "    PARAMS = {\n",
        "      \"action\": \"query\",\n",
        "      \"prop\": \"revisions\",\n",
        "      \"titles\": title,\n",
        "      \"rvlimit\": \"500\",\n",
        "      \"rvprop\": \"timestamp|ids|content\",\n",
        "      \"rvdir\": \"newer\",\n",
        "      \"rvstart\": \"2001-01-01\"+\"T00:00:00Z\",\n",
        "      \"formatversion\": \"2\",\n",
        "      \"format\": \"json\",\n",
        "      \"rvcontinue\": DATA['continue']['rvcontinue']\n",
        "    }\n",
        "\n",
        "    R = S.get(url=URL, params=PARAMS)\n",
        "    DATA = R.json()\n",
        "    for each in DATA['query']['pages']:\n",
        "      for revision in each['revisions']:\n",
        "        for term in terms:\n",
        "          if 'content' in revision.keys():\n",
        "            row = [title,'https://en.wikipedia.org/w/index.php?title='+title+'&oldid='+str(revision['revid']),revision['timestamp']]\n",
        "            #search for the term\n",
        "            if term.lower() in revision['content'].lower():\n",
        "              #and add a result to the data output if the term is found\n",
        "              row.append(term)\n",
        "              df.loc[len(df)] = row\n",
        "\n",
        "    # Dump the latest version of the reuslts\n",
        "    df.to_csv(dump_filename)\n",
        "    print('Queried another 500 revisions for ' + title + ' until '+revision['timestamp'])\n",
        "\n",
        "print('Done.')"
      ],
      "metadata": {
        "id": "HA_qnH03MRBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the CSV"
      ],
      "metadata": {
        "id": "aghyPKIpsHDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  df.to_csv(output_file, index = False, encoding='utf-8')\n",
        "  print('Done.')\n",
        "except IOError:\n",
        "  print(\"/!\\ Error while writing the output file\")"
      ],
      "metadata": {
        "id": "48D2D2llsItf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}