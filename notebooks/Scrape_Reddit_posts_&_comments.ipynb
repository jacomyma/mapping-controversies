{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacomyma/mapping-controversies/blob/main/notebooks/Scrape_Reddit_posts_%26_comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🍯 Scrape Reddit posts & comments"
      ],
      "metadata": {
        "id": "Z9TqsUaL7nYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import required libraries"
      ],
      "metadata": {
        "id": "fEXUZCm-n0Ya"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBnAC05xWv3c"
      },
      "outputs": [],
      "source": [
        "!pip install praw\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import csv\n",
        "import praw\n",
        "import datetime as dt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrive submission IDs\n",
        "In this cell you will be able to retrieve the submission id's from a given subreddit. You can specify subreddit and query in the bottom where the function is called.\n",
        "\n",
        "The output of this cell will be a file called loop.csv, which you can find in the file folder in the sidebar. Loop.csv contains all submission IDs matching your query.\n",
        "\n",
        "**If you wanna use multiple different queries make sure to save these files between running the cell, and combine them afterward! It might work by appending the new query to the list, but as a precaution make sure to backup the loop.csv**"
      ],
      "metadata": {
        "id": "aDWHsEbDoE9H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRyQ99N1Wv3h"
      },
      "outputs": [],
      "source": [
        "PUSHSHIFT_REDDIT_URL = \"https://api.pullpush.io/reddit\"\n",
        "\n",
        "def fetchObjects(**kwargs):\n",
        "    # Default paramaters for API query\n",
        "    params = {\n",
        "        \"sort_type\":\"created_utc\",\n",
        "        \"sort\":\"asc\",\n",
        "        \"size\": 1000,\n",
        "        }\n",
        "\n",
        "    # Add additional paramaters based on function arguments\n",
        "    for key,value in kwargs.items():\n",
        "        params[key] = value\n",
        "\n",
        "    # Print API query paramaters\n",
        "    print(params)\n",
        "\n",
        "    # Set the type variable based on function input\n",
        "    # The type can be \"comment\" or \"submission\", default is \"comment\"\n",
        "    type = \"comment\"\n",
        "    if 'type' in kwargs and kwargs['type'].lower() == \"submission\":\n",
        "        type = \"submission\"\n",
        "\n",
        "    # Perform an API request\n",
        "    r = requests.get(PUSHSHIFT_REDDIT_URL + \"/\" + type + \"/search/\", params=params, timeout=30)\n",
        "\n",
        "    # Check the status code, if successful, process the data\n",
        "    if r.status_code == 200:\n",
        "        response = json.loads(r.text)\n",
        "        data = response['data']\n",
        "        sorted_data_by_id = sorted(data, key=lambda x: int(x['id'],36))\n",
        "        return sorted_data_by_id\n",
        "\n",
        "def extract_reddit_data(**kwargs):\n",
        "    # Speficify the start timestamp\n",
        "    max_created_utc = 1672563620  # 01/12/2021 @ 11:29pm - start with this\n",
        "    max_id = 0\n",
        "\n",
        "    # Open a file for JSON output\n",
        "    with open('loop.csv','a') as f1:\n",
        "        writer=csv.writer(f1, delimiter=',',lineterminator='\\n',)\n",
        "\n",
        "        # While loop for recursive function\n",
        "        while 1:\n",
        "            nothing_processed = True\n",
        "            # Call the recursive function\n",
        "            objects = fetchObjects(**kwargs,after=max_created_utc)\n",
        "\n",
        "            # Loop the returned data, ordered by date\n",
        "            for object in objects:\n",
        "                id = int(object['id'],36)\n",
        "                if id > max_id:\n",
        "                    nothing_processed = False\n",
        "                    created_utc = object['created_utc']\n",
        "                    max_id = id\n",
        "                    if created_utc > max_created_utc: max_created_utc = created_utc\n",
        "                    # Output JSON data to the opened file\n",
        "                    row = [object['id']]\n",
        "                    writer.writerow(row)\n",
        "\n",
        "            # Exit if nothing happened\n",
        "            if nothing_processed: return\n",
        "            max_created_utc -= 1\n",
        "\n",
        "            # Sleep a little before the next recursive function call\n",
        "            time.sleep(1)\n",
        "\n",
        "# Start program by calling function with:\n",
        "# 1) Subreddit specified\n",
        "# 2) The type of data required (comment or submission)\n",
        "# 3) A query - if provided an empty string it will just extract\n",
        "extract_reddit_data(subreddit=\"teachers\",type=\"submission\", q='artificial intelligence')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the list of IDs\n",
        "Here we will input the loop.csv from the previous cell. If you wanna manually curate a list of submission you can load in the file here. Make sure to save your manually curated list of submission in a csv, and call the column: id.\n",
        "\n",
        "It will print the total amount of IDs in the file after loading the file."
      ],
      "metadata": {
        "id": "yb_E4iAmqwSI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDTmphAnWv3j"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"loop.csv\",\n",
        "                   sep=',',\n",
        "                   names=[\"id\"])\n",
        "\n",
        "\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieving the content and comments of a Reddit submission\n",
        "In this cell the actual content will be retrieved based on the ID. From the submission we'll retrieve: The title, the upvote score, the ID, the URL, number of comments, UNIX timestamp, the text content, and the username.\n",
        "\n",
        "Further, the first-level comments of the submission will also be retrieve. It is possible to add a recursive loop to this, but I haven't had the time. Experiment on your own.\n",
        "\n",
        "You might also have to create your own account and widget, if the account I have supplied becomes blocked. You can find information on this online.\n",
        "\n",
        "First you need to decide on a limit for how many times it should load in more comments. This is mainly in cases where there are a lot of comments on a post. A limit of None loads in more comments 0 times, while a limit of 2 loads in more comments two times."
      ],
      "metadata": {
        "id": "ulzPCr51rYwa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Wf7R7QuWv3j"
      },
      "outputs": [],
      "source": [
        "more_limit = None\n",
        "\n",
        "reddit = praw.Reddit(client_id=\"hhxzoVnl0x0jlA\",                      # client id\n",
        "                     client_secret=\"wPJjQSLNIeMbtK3xilkrHdW3ftToCw\",  # client secret\n",
        "                     user_agent=\"scraper\",                            # user agent name\n",
        "                     username = \"digitalscienceboi\",                  # reddit username\n",
        "                     password = \"htHejLGDFLYVrX6\",                    # reddit password\n",
        "                     check_for_async=False)\n",
        "\n",
        "post_dict = { \"title\":[],\"score\":[],\"id\":[], \"url\":[], \"comms_num\":[],\"created\":[],\"body\":[],\"author\":[]}\n",
        "comment_dict = {\"author\":[],\"score\":[],\"id\":[],\"created\":[],\"link_id\":[],\"body\":[]}\n",
        "\n",
        "#for x in data['id']:\n",
        "submission = (praw.models.Submission(reddit, id = 'tjb97k'))\n",
        "post_dict[\"title\"].append(submission.title)\n",
        "post_dict[\"score\"].append(submission.score)\n",
        "post_dict[\"id\"].append(submission.id)\n",
        "post_dict[\"url\"].append(submission.url)\n",
        "post_dict[\"comms_num\"].append(submission.num_comments)\n",
        "post_dict[\"created\"].append(submission.created)\n",
        "post_dict[\"body\"].append(submission.selftext)\n",
        "post_dict[\"author\"].append(submission.author)\n",
        "comments = submission.comments\n",
        "comments.replace_more(limit=more_limit)\n",
        "for comment in comments:\n",
        "  comment_dict[\"author\"].append(comment.author)\n",
        "  comment_dict[\"score\"].append(comment.score)\n",
        "  comment_dict[\"id\"].append(comment.id)\n",
        "  comment_dict[\"created\"].append(comment.created)\n",
        "  comment_dict[\"link_id\"].append(comment.link_id)\n",
        "  comment_dict[\"body\"].append(comment.body)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting UNIX timestamp to datetime\n",
        "In this cell the UNIX timestamp will be converted to a proper datetime (YYYY-MM-DD HH:MM:SS) that is more usable.\n",
        "\n",
        "This cell will also output two files: One for submissions (submissions.csv) and one for comments (comments.csv). These can be merged if you wanna do a semantic network."
      ],
      "metadata": {
        "id": "QYs9YYJTsqCm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXj7myxPWv3k"
      },
      "outputs": [],
      "source": [
        "def get_date(created):\n",
        "    return dt.datetime.fromtimestamp(created)\n",
        "\n",
        "post_out = pd.DataFrame(post_dict)\n",
        "comment_out = pd.DataFrame(comment_dict)\n",
        "\n",
        "post_timestamp = post_out['created'].apply(get_date)\n",
        "comment_timestamp = comment_out['created'].apply(get_date)\n",
        "\n",
        "post_out = post_out.assign(timestamp = post_timestamp)\n",
        "comment_out = comment_out.assign(timestamp = comment_timestamp)\n",
        "\n",
        "post_out.to_csv('submissions.csv')\n",
        "comment_out.to_csv('comments.csv')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}