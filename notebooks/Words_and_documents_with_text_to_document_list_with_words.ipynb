{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Words and documents with text to document list with words",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacomyma/mapping-controversies/blob/main/notebooks/Words_and_documents_with_text_to_document_list_with_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’ Words and documents with text to document list with words"
      ],
      "metadata": {
        "id": "JusW97SUGBgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inputs:**\n",
        "* a list of documents with their text content (CSV)\n",
        "* a small list of words, like a dozen (CSV)\n",
        "\n",
        "**Outputs:**\n",
        "* a list of documents with words as columns (CSV)\n",
        "* a list of document-word pairs (CSV)\n",
        "* a bipartite network of documents and words (GEXF)\n",
        "\n",
        "This script tells you which words are in which documents. Each word becomes a column, that is why you want to only have a few of them. You may have many documents, though. Words can be expressions (e.g., named entities).\n",
        "\n",
        "If you have many words and just want the network, check [this notebook](https://colab.research.google.com/github/jacomyma/mapping-controversies/blob/main/notebooks/Words_and_documents_with_text_to_network.ipynb)."
      ],
      "metadata": {
        "id": "31PaqnfcGDWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use"
      ],
      "metadata": {
        "id": "cVzHKii6HvCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Put your input files in the same folder as the notebook\n",
        "1. Edit the settings if needed. CHECK THE COLUMN NAMES!\n",
        "1. Run all the cells\n",
        "1. Take ALL the output files from the notebook folder"
      ],
      "metadata": {
        "id": "yiVPSZzJHz22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETTINGS"
      ],
      "metadata": {
        "id": "2aMAjhGDH2qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input file 1: documents\n",
        "input_file_documents = \"documents.csv\"\n",
        "# Which column contains the text?\n",
        "documents_text_column = \"Text\"\n",
        "# Which column contains the document name or ID?\n",
        "documents_id_column = \"Article\"\n",
        "\n",
        "# Input file 2: small list of words\n",
        "input_file_words = \"words-small-list.csv\"\n",
        "# Which column contains the words?\n",
        "words_text_column = \"text\"\n",
        "\n",
        "# Delete documents that contain none of the words?\n",
        "discard_unrelated_documents = True\n",
        "\n",
        "# Output files\n",
        "output_file_documents = \"documents-with-terms.csv\"\n",
        "output_file_pairs = \"terms-and-documents.csv\"\n",
        "output_file_network = \"terms-document-network.gexf\"\n"
      ],
      "metadata": {
        "id": "ODgqeaStH3kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCRIPT"
      ],
      "metadata": {
        "id": "9d_s_6zbH40x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import libraries\n",
        "This notebook draws on existing code.\n",
        "You can ignore the output."
      ],
      "metadata": {
        "id": "Swaxx6G4H8yS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU-bf9BtvSJN"
      },
      "source": [
        "# Install (if needed)\n",
        "!pip install pandas\n",
        "!pip install spacy\n",
        "!pip install networkx\n",
        "\n",
        "# Import\n",
        "import csv\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the input file 1 (documents)"
      ],
      "metadata": {
        "id": "d0FSMimJIZtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_df = pd.read_csv(input_file_documents, quotechar='\"', encoding='utf8', doublequote=True, quoting=csv.QUOTE_NONNUMERIC, dtype=object, keep_default_na=False)\n",
        "print(\"Preview of the document list:\")\n",
        "doc_df"
      ],
      "metadata": {
        "id": "3OF7BDMiIbok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the input file 2 (words)"
      ],
      "metadata": {
        "id": "U9uSFdYiKoAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_df = pd.read_csv(input_file_words, quotechar='\"', encoding='utf8', doublequote=True, quoting=csv.QUOTE_NONNUMERIC, dtype=object, keep_default_na=False)\n",
        "print(\"Preview of the word list:\")\n",
        "word_df"
      ],
      "metadata": {
        "id": "FXNwsPgVKlWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wrangle the data"
      ],
      "metadata": {
        "id": "eczPM9TFMO3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a set of the words\n",
        "words = set()\n",
        "for index, row in word_df.iterrows():\n",
        "  words.add(row[words_text_column])\n",
        "\n",
        "# Init data for output\n",
        "document_list = []\n",
        "pair_list = []\n",
        "network_doc_set = set()\n",
        "network_word_set = set()\n",
        "network_edge_list = []\n",
        "\n",
        "# Search words in documents\n",
        "for index, row in doc_df.iterrows():\n",
        "  text = row[documents_text_column].lower()\n",
        "  count_per_word = {}\n",
        "  flag = False\n",
        "  for word in words:\n",
        "    count = text.count(word.lower())\n",
        "    count_per_word[word] = count\n",
        "    if count > 0:\n",
        "      flag = True\n",
        "\n",
        "  if flag or not discard_unrelated_documents:\n",
        "    # output 1\n",
        "    doc_new_row = {**row, **count_per_word}\n",
        "    document_list.append(doc_new_row)\n",
        "    # output 2\n",
        "    for word in words:\n",
        "      count = count_per_word[word]\n",
        "      if count > 0:\n",
        "        pair_new_row = {**row, 'term':word, 'term-count':count}\n",
        "        pair_list.append(pair_new_row)\n",
        "    # output 3\n",
        "    doc_id = row[documents_id_column]\n",
        "    network_doc_set.add(doc_id)\n",
        "    for word in words:\n",
        "      count = count_per_word[word]\n",
        "      if count > 0:\n",
        "        network_word_set.add(word)\n",
        "        network_edge_list.append((doc_id,word,{\"count\":count}))\n",
        "\n"
      ],
      "metadata": {
        "id": "HA_qnH03MRBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make output 1 (documents with words as columns)"
      ],
      "metadata": {
        "id": "0ghnk14YPSm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_doc_df = pd.DataFrame(document_list)\n",
        "output_doc_df = output_doc_df.drop(columns=[documents_text_column])\n",
        "print(\"Done.\")\n",
        "print(\"Preview of the document list:\")\n",
        "output_doc_df"
      ],
      "metadata": {
        "id": "UErfQTMAPW5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make output 2 (document-word pairs)"
      ],
      "metadata": {
        "id": "w_wdfLTdRWe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_pair_df = pd.DataFrame(pair_list)\n",
        "output_pair_df = output_pair_df.drop(columns=[documents_text_column])\n",
        "print(\"Done.\")\n",
        "print(\"Preview of the pair list:\")\n",
        "output_pair_df"
      ],
      "metadata": {
        "id": "gwO-C1wARaj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the CSVs"
      ],
      "metadata": {
        "id": "x57hmk7DZlbq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiJ2zmLiUKYB"
      },
      "source": [
        "try:\n",
        "  output_doc_df.to_csv(output_file_documents, index = False, encoding='utf-8')\n",
        "except IOError:\n",
        "  print(\"/!\\ Error while writing the documents output file\")\n",
        "\n",
        "try:\n",
        "  output_pair_df.to_csv(output_file_pairs, index = False, encoding='utf-8')\n",
        "except IOError:\n",
        "  print(\"/!\\ Error while writing the pairs output file\")\n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make and save output 3 (network)"
      ],
      "metadata": {
        "id": "jdfseWV4SH8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the nodes\n",
        "nodes = []\n",
        "doc_df_no_text = doc_df.drop(columns=[documents_text_column])\n",
        "for index, row in doc_df_no_text.iterrows():\n",
        "  if row[documents_id_column] in network_doc_set:\n",
        "    nodes.append((row[documents_id_column], {**row, 'label':row[documents_id_column], 'type':'document'}))\n",
        "\n",
        "for index, row in word_df.iterrows():\n",
        "  if row[words_text_column] in network_word_set:\n",
        "    nodes.append((row[words_text_column], {**row, 'label':row[words_text_column], 'type':'term'}))\n",
        "\n",
        "# Build edges\n",
        "edges = network_edge_list\n",
        "\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(nodes)\n",
        "G.add_edges_from(edges)\n",
        "nx.write_gexf(G, output_file_network)\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "P4pxNWmASJ6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}