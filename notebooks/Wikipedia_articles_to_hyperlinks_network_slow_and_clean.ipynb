{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wikipedia articles to hyperlinks network (slow and clean).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM2ypAbcA2+I6iRT04EbISc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacomyma/mapping-controversies/blob/main/notebooks/Wikipedia_articles_to_hyperlinks_network_slow_and_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üç£ Wikipedia articles to hyperlinks network (slow and clean)\n"
      ],
      "metadata": {
        "id": "YZgEmF31L15o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Input:** a list of Wikipedia articles (CSV).\n",
        "\n",
        "**Output:** a network of Wikipedia articles connected by hyperlinks (GEXF).\n",
        "\n",
        "This scripts queries Wikipedia for each article of the input list. It retrieves the hyperlinks for each article. Then it outputs a directed network where the nodes are exactly the articles of the input list, and the edges are the hyperlinks between these articles (if any).\n",
        "\n",
        "*Note 1: some articles of the list might be redirections to other articles of the list, therefore the output may have less articles than the input.*\n",
        "\n",
        "*Note 2: this version of the script is dubbed \"slow and clean\" because it only takes the hyperlinks from the actual content of the article. It requires more parsing and is thus slower, but the links are more relevant because it does not include the article footer. If you do not have the time, [use the quick and dirty version instead](https://colab.research.google.com/github/jacomyma/mapping-controversies/blob/main/notebooks/Wikipedia_articles_to_hyperlinks_network_quick_and_dirty.ipynb).*"
      ],
      "metadata": {
        "id": "A8Ve0dJkN_P4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use"
      ],
      "metadata": {
        "id": "CoKQIdi1OBEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Put your input file in the same folder as the notebook\n",
        "1. Edit the settings if needed\n",
        "1. Run all the cells\n",
        "1. Take the output file from the notebook folder"
      ],
      "metadata": {
        "id": "wJiVf6HUOCh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETTINGS"
      ],
      "metadata": {
        "id": "eP1JxwI_OHOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input file\n",
        "input_file = \"wikipedia-articles.csv\"\n",
        "\n",
        "# Which column contains the article title?\n",
        "article_name_column = \"Article\"\n",
        "\n",
        "# Output file\n",
        "output_file = \"Wikipedia-articles-hyperlinks-network.GEXF\""
      ],
      "metadata": {
        "id": "lP5YhbuNOI9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCRIPT"
      ],
      "metadata": {
        "id": "DBKPInzVOjze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import libraries\n",
        "This notebook draws on existing code.\n",
        "You can ignore the output."
      ],
      "metadata": {
        "id": "0wtbVwVxBXbN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHnvGi5LAzo2"
      },
      "outputs": [],
      "source": [
        "# Install (if needed)\n",
        "!pip install wikipedia-api\n",
        "!pip install pandas\n",
        "!pip install networkx\n",
        "\n",
        "# Import\n",
        "import wikipediaapi\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import csv\n",
        "\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the input file"
      ],
      "metadata": {
        "id": "9nfAokf3eoxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article_df = pd.read_csv(input_file, quotechar='\"', encoding='utf8', doublequote=True, quoting=csv.QUOTE_NONNUMERIC, dtype=object)\n",
        "print(\"Preview of the article list:\")\n",
        "article_df"
      ],
      "metadata": {
        "id": "F2rDe2ChequF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Harvest Wikipedia"
      ],
      "metadata": {
        "id": "XFhCP7YYCGKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is an object we use to connect to the API.\n",
        "# Note that we configure it to use the English Wikipedia.\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "  language='en',\n",
        "  extract_format = wikipediaapi.ExtractFormat.WIKI,\n",
        "  user_agent='ControversyMapping/0.0 (https://jacomyma.github.io/mapping-controversies/)'\n",
        ")\n",
        "\n",
        "seen = []\n",
        "network = {}\n",
        "print(\"Harvesting all links from \"+str(len(article_df.index))+\" wikipedia pages. This might take a while...\")\n",
        "count=1\n",
        "\n",
        "# Harvest each article one by one\n",
        "for title in article_df[article_name_column]:\n",
        "  if count % 50 == 0:\n",
        "    print(\"All links harvested from \"+str(count)+\" pages out of \"+str(len(article_df.index))+\". Continuing...\")\n",
        "  if not title in seen: # Do not harvest twice the same...\n",
        "    seen.append(title)\n",
        "    try:\n",
        "      page = wiki_wiki.page(title)\n",
        "      text_links = []\n",
        "      links = page.links\n",
        "      for link_title in sorted(links.keys()):\n",
        "        text_links.append(link_title)\n",
        "      network.update({title:text_links})\n",
        "\n",
        "    except:\n",
        "        print('SKIPPED: '+title+' (an error occurred)')\n",
        "  count=count+1\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "-dsi13FiCeZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build network"
      ],
      "metadata": {
        "id": "82RyJ-abiR3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building network...\")\n",
        "\n",
        "# Build the nodes, with attributes\n",
        "nodes = []\n",
        "for index, row in article_df.iterrows():\n",
        "  nodes.append((row[article_name_column], {**row, 'label':row[article_name_column]}))\n",
        "\n",
        "# Build edges (only between source nodes)\n",
        "edges = []\n",
        "source_node_set = network.keys()\n",
        "for source in network:\n",
        "  for target in network[source]:\n",
        "    edge = (source,target)\n",
        "    if target in source_node_set:\n",
        "      edges.append(edge)\n",
        "print(\"Saving network...\")\n",
        "\n",
        "# Assemble\n",
        "G = nx.DiGraph()\n",
        "G.add_nodes_from(nodes)\n",
        "G.add_edges_from(edges)\n",
        "nx.write_gexf(G, output_file)\n",
        "\n",
        "print('Done.')"
      ],
      "metadata": {
        "id": "PBj9EeAzfkR6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}