{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNpznlqnhEfyFFovbK56fV1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacomyma/mapping-controversies/blob/main/notebooks/Scrape_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üêî Scrape tweets"
      ],
      "metadata": {
        "id": "zBjunangCj6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input:** a query (like in Twitter search), a start date, and an end date.\n",
        "\n",
        "**Output:** a list of tweets with metadata (CSV).\n",
        "\n",
        "This script scrapes tweets (no API key needed). It does it **day per day**, with a fixed maximum number of tweets per day. Therefore it is **not** appropriate for tracking the number of tweets per day. It works well to harvest a comparable number of tweets per day and follow their profile (mentions, hashtags, etc.)."
      ],
      "metadata": {
        "id": "vy3c2d0WC2Eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use"
      ],
      "metadata": {
        "id": "SF6PTbdTFurR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Edit the settings\n",
        "1. Run all the cells\n",
        "1. Take the output file from the notebook folder"
      ],
      "metadata": {
        "id": "Ze4ilXdSFvmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to build a Twitter query?\n",
        "* You could use a single word, or a hashtag\n",
        "* You can use AND, OR and parentheses\n",
        "* You can search media-specific fields: check the [official documentation](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query).\n",
        "* Do not use \"since:\" and \"until:\" in the query, because we deal with time a different way."
      ],
      "metadata": {
        "id": "8S24FKsCHP9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETTINGS"
      ],
      "metadata": {
        "id": "ix7s4cCIGBsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Settings\n",
        "search_query = '#chatgpt'\n",
        "since = '2023-03-01'\n",
        "until = '2023-03-10'\n",
        "max_results_per_day = 100\n",
        "\n",
        "# Output file\n",
        "output_file = \"tweets.csv\""
      ],
      "metadata": {
        "id": "TBEDbxBuOYZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCRIPT"
      ],
      "metadata": {
        "id": "vlEcvNUuGDUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import libraries\n",
        "This notebook draws on existing code.\n",
        "You can ignore the output."
      ],
      "metadata": {
        "id": "2MSLwauQGHYi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXj9KJh1M-Yk"
      },
      "outputs": [],
      "source": [
        "# Install and import necessary libraries\n",
        "!pip install snscrape # SNScrape - the Twitter scraping library\n",
        "import pandas as pd # Pandas: deal with the data\n",
        "import snscrape.modules.twitter as sntwitter # SNScrape: get Twitter data\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metadata of the tweets that we keep"
      ],
      "metadata": {
        "id": "So1cY9cRGwMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tweet features to keep in the output\n",
        "features = [\n",
        "  \"url\",\n",
        "  \"date\",\n",
        "  \"id\",\n",
        "  \"user\",\n",
        "  #\"card\",\n",
        "  #\"cashtags\",\n",
        "  \"coordinates\",\n",
        "  \"hashtags\",\n",
        "  \"inReplyToTweetId\",\n",
        "  \"inReplyToUser\",\n",
        "  \"media\",\n",
        "  \"mentionedUsers\",\n",
        "  \"place\",\n",
        "  \"quotedTweet\",\n",
        "  \"retweetedTweet\",\n",
        "  \"sourceLabel\",\n",
        "  #\"sourceUrl\",\n",
        "  #\"renderedContent\",\n",
        "  \"replyCount\",\n",
        "  \"retweetCount\",\n",
        "  \"likeCount\",\n",
        "  \"quoteCount\",\n",
        "  \"conversationId\",\n",
        "  \"lang\",\n",
        "  #\"source\"\n",
        "]"
      ],
      "metadata": {
        "id": "u7n08EFDR-tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scrape the tweets"
      ],
      "metadata": {
        "id": "05OGMEhzGTGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = []\n",
        "currentDate = datetime.date.fromisoformat(since)\n",
        "maxDate = datetime.date.fromisoformat(until)\n",
        "while currentDate<maxDate:\n",
        "  nextDate = currentDate + datetime.timedelta(days=1)\n",
        "  print(\"# Harvesting new day \"+str(currentDate))\n",
        "  # Harvest the tweets\n",
        "  query = \"since:\"+str(currentDate)+\" until:\"+str(nextDate)+\" \"+search_query\n",
        "  print(\"QUERY: \"+query)\n",
        "  scraper = sntwitter.TwitterSearchScraper(query)\n",
        "  for i, tweet in enumerate(scraper.get_items()):\n",
        "    data = []\n",
        "    for f in features:\n",
        "      if f == \"card\":\n",
        "        if tweet.card:\n",
        "          data.append(tweet.card.title)\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"cashtags\":\n",
        "        data.append(\";\".join(tweet.cashtags))\n",
        "      elif f == \"coordinates\":\n",
        "        if tweet.coordinates:\n",
        "          data.append(\"{};{}\".format(tweet.coordinates.latitude, tweet.coordinates.longitude))\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"hashtags\":\n",
        "        if tweet.hashtags:\n",
        "          data.append(\";\".join(tweet.hashtags))\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"inReplyToTweetId\":\n",
        "        if tweet.inReplyToTweetId:\n",
        "          data.append(tweet.inReplyToTweetId)\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"inReplyToUser\":\n",
        "        if tweet.inReplyToUser:\n",
        "          data.append(str(tweet.inReplyToUser).replace(\"https://twitter.com/\", \"\"))\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"media\":\n",
        "        if tweet.media:\n",
        "          medias = []\n",
        "          for m in tweet.media:\n",
        "            if hasattr(m,\"fullUrl\"):\n",
        "              medias.append(m.fullUrl)\n",
        "            elif hasattr(m,\"thumbnailUrl\"):\n",
        "              medias.append(m.thumbnailUrl)\n",
        "            elif hasattr(m,\"url\"):\n",
        "              medias.append(m.url)\n",
        "            else:\n",
        "              medias.append(\"MISSING\")\n",
        "          data.append(\";\".join(medias))\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"mentionedUsers\":\n",
        "        if tweet.mentionedUsers:\n",
        "          data.append(\";\".join([u.username for u in tweet.mentionedUsers]))\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"place\":\n",
        "        if tweet.place:\n",
        "          data.append(tweet.place.fullName)\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"quotedTweet\":\n",
        "        if tweet.quotedTweet:\n",
        "          data.append(tweet.quotedTweet)\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"retweetedTweet\":\n",
        "        if tweet.retweetedTweet:\n",
        "          data.append(tweet.retweetedTweet)\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"sourceLabel\":\n",
        "        if tweet.sourceLabel:\n",
        "          data.append(tweet.sourceLabel)\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"sourceUrl\":\n",
        "        if tweet.sourceUrl:\n",
        "          data.append(tweet.sourceUrl)\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"url\":\n",
        "        if tweet.url:\n",
        "          data.append(tweet.url)\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"date\":\n",
        "        if tweet.date:\n",
        "          data.append(tweet.date)\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"renderedContent\":\n",
        "        if tweet.renderedContent:\n",
        "          data.append(tweet.renderedContent)\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"id\":\n",
        "        if tweet.id:\n",
        "          data.append(tweet.id)\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"user\":\n",
        "        if tweet.user:\n",
        "          data.append(tweet.user.username.replace(\"https://twitter.com/\", \"\"))\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"replyCount\":\n",
        "        if tweet.replyCount:\n",
        "          data.append(str(tweet.replyCount))\n",
        "        else:\n",
        "          data.append(\"0\")\n",
        "      elif f == \"retweetCount\":\n",
        "        if tweet.retweetCount:\n",
        "          data.append(str(tweet.retweetCount))\n",
        "        else:\n",
        "          data.append(\"0\")\n",
        "      elif f == \"likeCount\":\n",
        "        if tweet.likeCount:\n",
        "          data.append(str(tweet.likeCount))\n",
        "        else:\n",
        "          data.append(\"0\")\n",
        "      elif f == \"quoteCount\":\n",
        "        if tweet.quoteCount:\n",
        "          data.append(str(tweet.quoteCount))\n",
        "        else:\n",
        "          data.append(\"0\")\n",
        "      elif f == \"conversationId\":\n",
        "        if tweet.conversationId:\n",
        "          data.append(tweet.conversationId)\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"lang\":\n",
        "        if tweet.lang:\n",
        "          data.append(tweet.lang)\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "      elif f == \"source\":\n",
        "        if tweet.source:\n",
        "          data.append(tweet.source)\n",
        "        else:\n",
        "          data.append(\"\")\n",
        "\n",
        "    tweets.append(data)\n",
        "\n",
        "    if i>0 and i%1000 == 0:\n",
        "      print(\"  \"+str(i)+\" tweets harvested for that day...\")\n",
        "\n",
        "    if i > max_results_per_day:\n",
        "      break\n",
        "      \n",
        "  currentDate = nextDate\n",
        "  print(\"\")\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "Y6BPPaJmzVrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save as a CSV"
      ],
      "metadata": {
        "id": "EFofHSJgG6kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the list of tweets to a DataFrame and save it to a CSV file\n",
        "tweetdf = pd.DataFrame(tweets, columns=features)\n",
        "try:\n",
        "  tweetdf.to_csv(output_file, index = False, encoding='utf-8', sep=',')\n",
        "except IOError:\n",
        "  print(\"/!\\ Error while writing the output file\")\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "v4IT-6_ANLe0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}