{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wikipedia articles to articles and editors network",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLvUaOFliaqt+sgBvJIUji",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacomyma/mapping-controversies/blob/main/notebooks/Wikipedia_articles_to_articles_and_editors_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üçÑ Wikipedia articles to articles and editors network"
      ],
      "metadata": {
        "id": "J0Sr179sYEKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input:** a list of Wikipedia articles (CSV).\n",
        "\n",
        "**Output:** a bipartite network of articles and editors connected when the editor edited that article (GEXF).\n",
        "\n",
        "This scripts queries Wikipedia for the list of edits (i.e. revisions) for each article of the input list. Then it takes the editors of these edits, and makes a network where nodes are of two types: articles, and editors. You may set a time range. In that case, some revisions will not be taken into account, and that will ignore certain editors."
      ],
      "metadata": {
        "id": "UC5LToKXYJpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use"
      ],
      "metadata": {
        "id": "mtRw0Yh8YYLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Put your input file in the same folder as the notebook\n",
        "1. Edit the settings if needed\n",
        "1. Run all the cells\n",
        "1. Take the output file from the notebook folder"
      ],
      "metadata": {
        "id": "laN_kVbNYYm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETTINGS"
      ],
      "metadata": {
        "id": "kN6XuSe_dsMs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNi1By5NYCcs"
      },
      "outputs": [],
      "source": [
        "# Input file\n",
        "input_file = \"wikipedia-articles.csv\"\n",
        "\n",
        "# Which column contains the article title?\n",
        "article_name_column = \"Article\"\n",
        "\n",
        "# Start date\n",
        "start_date=\"2000-01-01\"\n",
        "\n",
        "# End date\n",
        "end_date=\"2100-01-01\"\n",
        "\n",
        "# Output file\n",
        "output_file = \"wikipedia-articles-editors-network.gexf\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCRIPT"
      ],
      "metadata": {
        "id": "GhL_BGTbd1Mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import libraries\n",
        "This notebook draws on existing code.\n",
        "You can ignore the output."
      ],
      "metadata": {
        "id": "Q2vEj1G1d25v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In this cell Jupyter checks whether you have the right libraries installed \n",
        "\n",
        "import sys\n",
        "\n",
        "try: #First, Jupyter tries to import a library\n",
        "  import requests\n",
        "  print(\"Requests library has been imported\")\n",
        "except: #If it fails, it will try to install the library\n",
        "  print(\"Requests library not found. Installing...\")\n",
        "  !pip install requests\n",
        "  try:#... and try to import it again\n",
        "    import requests\n",
        "  except: #unless it fails, and raises an error.\n",
        "    print(\"Something went wrong in the installation of the requests library. Please check your internet connection and consult output from the installation below\")\n",
        "try:\n",
        "  import networkx as nx\n",
        "  print(\"NetworkX library has been imported\")\n",
        "except:\n",
        "  print(\"NetworkX library not found. Installing...\")\n",
        "  !pip install networkx\n",
        "  \n",
        "  try:\n",
        "    import networkx as nx\n",
        "  except:\n",
        "    print(\"Something went wrong in the installation of the NetworkX library. Please check your internet connection and consult output from the installation below\")\n",
        "\n",
        "\n",
        "# Install (if needed)\n",
        "!pip install pandas\n",
        "\n",
        "# Import\n",
        "import pandas as pd\n",
        "import csv"
      ],
      "metadata": {
        "id": "UF-lH31dd4q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the input file"
      ],
      "metadata": {
        "id": "li-nOSF2ewKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article_df = pd.read_csv(input_file, quotechar='\"', encoding='utf8', doublequote=True, quoting=csv.QUOTE_NONNUMERIC, dtype=object)\n",
        "print(\"Preview of the article list:\")\n",
        "article_df"
      ],
      "metadata": {
        "id": "h6bZkpLmewnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Harvest the list of edits"
      ],
      "metadata": {
        "id": "5RLY9g9Otuig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Language\n",
        "lan = \"en\"\n",
        "\n",
        "print(\"Harvesting data from \"+str(len(article_df.index))+\" input pages...\")\n",
        "S = requests.Session()\n",
        "count=1\n",
        "data_index = {}\n",
        "for title in article_df[article_name_column]:\n",
        "  Revisions = []\n",
        "  URL = \"http://\"+lan+\".wikipedia.org/w/api.php\"\n",
        "  if count % 50 == 0:\n",
        "    print(\"Data harvested from \"+str(count)+\" articles out of \"+str(len(article_df.index))+\". Continuing harvest...\")\n",
        "  PARAMS = {\n",
        "    \"action\": \"query\",\n",
        "    \"prop\": \"revisions\",\n",
        "    \"titles\": title,\n",
        "    \"rvlimit\": \"500\",\n",
        "    \"rvprop\": \"user|timestamp\",\n",
        "    \"rvdir\": \"newer\",\n",
        "    \"rvstart\": start_date+\"T00:00:00Z\",\n",
        "    \"rvend\": end_date+\"T00:00:00Z\",\n",
        "    \"formatversion\": \"2\",\n",
        "    \"format\": \"json\"\n",
        "  }\n",
        "\n",
        "  R = S.get(url=URL, params=PARAMS)\n",
        "  if R.status_code==404:\n",
        "    print(\"The page does not exist. Skipping...\")\n",
        "    continue\n",
        "  DATA = R.json()\n",
        "  for each in DATA['query']['pages']:\n",
        "    Revisions.append(each)\n",
        "\n",
        "  while 'continue' in DATA.keys():\n",
        "    PARAMS = {\n",
        "      \"action\": \"query\",\n",
        "      \"prop\": \"revisions\",\n",
        "      \"titles\": title,\n",
        "      \"rvlimit\": \"500\",\n",
        "      \"rvprop\": \"user|timestamp\",\n",
        "      \"rvdir\": \"newer\",\n",
        "      \"rvstart\": start_date+\"T00:00:00Z\",\n",
        "      \"rvend\": end_date+\"T00:00:00Z\",\n",
        "      \"formatversion\": \"2\",\n",
        "      \"format\": \"json\",\n",
        "      \"rvcontinue\": DATA['continue']['rvcontinue']\n",
        "    }\n",
        "\n",
        "    R = S.get(url=URL, params=PARAMS)\n",
        "    DATA = R.json()\n",
        "    for each in DATA['query']['pages']:\n",
        "      Revisions.append(each)\n",
        "\n",
        "  for each in Revisions:\n",
        "    if \"revisions\" in each:\n",
        "      for every in each[\"revisions\"]:\n",
        "        if \"user\" in every:\n",
        "          user = every[\"user\"]\n",
        "          if not user in data_index:\n",
        "            data_index[user] = {}\n",
        "          if not title in data_index[user]:\n",
        "            data_index[user][title] = 0\n",
        "          data_index[user][title] += 1\n",
        "  count=count+1\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "KaQYdytctb8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build bipartite network"
      ],
      "metadata": {
        "id": "tHxd4EDUv_RU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building network...\")\n",
        "\n",
        "# Build the nodes\n",
        "nodes = []\n",
        "for index, row in article_df.iterrows():\n",
        "  nodes.append((row[article_name_column], {**row, 'label':row[article_name_column], 'type':'article'}))\n",
        "for user in data_index.keys():\n",
        "  nodes.append((user, {'label':user, 'type':'editor'}))\n",
        "\n",
        "# Build edges\n",
        "edges = []\n",
        "for user in data_index.keys():\n",
        "  for title in data_index[user].keys():\n",
        "    edge = (user,title,{\"count\":data_index[user][title]})\n",
        "    edges.append(edge)\n",
        "\n",
        "print(\"Network has been generated. Saving...\")\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(nodes)\n",
        "G.add_edges_from(edges)\n",
        "nx.write_gexf(G, output_file)\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "Ky92wgZOuqfE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}