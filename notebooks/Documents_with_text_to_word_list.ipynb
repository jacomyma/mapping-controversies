{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Documents with text to word list",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacomyma/mapping-controversies/blob/main/notebooks/Documents_with_text_to_word_list.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üçï Documents with text to word list"
      ],
      "metadata": {
        "id": "JusW97SUGBgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input:** a list of documents with their text content (CSV).\n",
        "\n",
        "**Outputs:**\n",
        "* a list of words with scores (CSV)\n",
        "* a list of document-word pairs (CSV)\n",
        "\n",
        "This scripts extracts so-called [named entities](https://en.wikipedia.org/wiki/Named-entity_recognition): words or groups of words that are person names, organizations, locations...\n",
        "\n",
        "The **description of the types** is displayed in the SCRIPT section of the notebook (execute it first)."
      ],
      "metadata": {
        "id": "31PaqnfcGDWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use"
      ],
      "metadata": {
        "id": "cVzHKii6HvCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Put your input file in the same folder as the notebook\n",
        "1. Edit the settings if needed\n",
        "1. Run all the cells\n",
        "1. Take ALL the output files from the notebook folder"
      ],
      "metadata": {
        "id": "yiVPSZzJHz22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETTINGS"
      ],
      "metadata": {
        "id": "2aMAjhGDH2qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input file\n",
        "input_file = \"documents.csv\"\n",
        "\n",
        "# Which column contains the text?\n",
        "text_column = \"Text\"\n",
        "\n",
        "# Use named entities\n",
        "# (if set to False, it just extracts words)\n",
        "# (don't extract all words if you have many documents!)\n",
        "use_named_entities = True\n",
        "\n",
        "# Use high quality named entities recognition model (slower, takes space)\n",
        "high_quality_model = False\n",
        "\n",
        "# Output files\n",
        "output_file_words = \"words.csv\"\n",
        "output_file_pairs = \"words-and-documents.csv\"\n"
      ],
      "metadata": {
        "id": "ODgqeaStH3kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCRIPT"
      ],
      "metadata": {
        "id": "9d_s_6zbH40x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import libraries\n",
        "This notebook draws on existing code.\n",
        "You can ignore the output."
      ],
      "metadata": {
        "id": "Swaxx6G4H8yS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU-bf9BtvSJN"
      },
      "source": [
        "# Install (if needed)\n",
        "!pip install pandas\n",
        "!pip install spacy\n",
        "\n",
        "# Import\n",
        "import csv\n",
        "import pandas as pd\n",
        "import spacy\n",
        "if use_named_entities and high_quality_model:\n",
        "  spacy.cli.download(\"en_core_web_lg\")\n",
        "\n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the input file"
      ],
      "metadata": {
        "id": "d0FSMimJIZtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_df = pd.read_csv(input_file, quotechar='\"', encoding='utf8', doublequote=True, quoting=csv.QUOTE_NONNUMERIC, dtype=object, on_bad_lines='skip')\n",
        "print(\"Preview of the document list:\")\n",
        "doc_df"
      ],
      "metadata": {
        "id": "3OF7BDMiIbok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract named entities\n",
        "We use spacy. More fun stuff to do [there](https://www.analyticsvidhya.com/blog/2021/06/nlp-application-named-entity-recognition-ner-in-python-with-spacy/)."
      ],
      "metadata": {
        "id": "VO8xIThdcA54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if use_named_entities:\n",
        "  # Named entities recognition engine\n",
        "  if high_quality_model:\n",
        "    NER = spacy.load(\"en_core_web_lg\")\n",
        "  else:\n",
        "    NER = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "  doc_index = {}\n",
        "  count=1\n",
        "  print(\"Extracting named entities from \"+str(len(doc_df.index))+\" documents. This might take a while...\")\n",
        "  for index, row in doc_df.iterrows():\n",
        "    text = row[text_column]\n",
        "    if count % 10 == 0:\n",
        "      print(\"Named entities extracted from \"+str(count)+\" documents out of \"+str(len(doc_df.index))+\". Continuing...\")\n",
        "    count += 1\n",
        "    \n",
        "    nertxt = NER(text)\n",
        "    entities = {}\n",
        "    for ne in nertxt.ents:\n",
        "      entsign = ne.text + '-' + ne.label_\n",
        "      if entsign not in entities:\n",
        "        entities[entsign] = {'NE-text': ne.text, 'NE-type':ne.label_, 'NE-count':0}\n",
        "      entities[entsign]['NE-count'] += 1\n",
        "    doc_index[index] = entities\n",
        "else:\n",
        "  # Tokenizer (extract words)\n",
        "  from spacy.lang.en import English\n",
        "  spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "  NLP = English()\n",
        "  doc_index = {}\n",
        "  count=1\n",
        "  print(\"Extracting words from \"+str(len(doc_df.index))+\" documents. This might take a while...\")\n",
        "  for index, row in doc_df.iterrows():\n",
        "    text = row[text_column]\n",
        "    if count % 10 == 0:\n",
        "      print(\"Text extracted from \"+str(count)+\" documents out of \"+str(len(doc_df.index))+\". Continuing...\")\n",
        "    count += 1\n",
        "    \n",
        "    tokens = NLP(text)\n",
        "    words = {}\n",
        "    for token in tokens:\n",
        "      if not token.is_stop and token.is_alpha:\n",
        "        t = token.text\n",
        "        if t not in words:\n",
        "          words[t] = {'W-text': t, 'W-count':0}\n",
        "        words[t]['W-count'] += 1\n",
        "    doc_index[index] = words\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "54qcGNXdXPLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the named entity types\n",
        "The library does not explain the types in its documentation, because it depends on the models. But the explanation is actually embedded in the library itself. The code below outputs the meaning of the types."
      ],
      "metadata": {
        "id": "aAZ8fxIbw_1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if use_named_entities:\n",
        "  types = set()\n",
        "  for index, row in doc_df.iterrows():\n",
        "    for entsign in doc_index[index]:\n",
        "      ne = doc_index[index][entsign]\n",
        "      types.add(ne['NE-type'])\n",
        "  print('Explanation of the types:')\n",
        "  for t in types:\n",
        "    print(' - '+t+': '+spacy.explain(t))\n",
        "else:\n",
        "  print('Done. (this is only relevant to named entities)')"
      ],
      "metadata": {
        "id": "fQaecQWawmh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregate words into dataframe"
      ],
      "metadata": {
        "id": "NQYXoKONeugn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_notxt_df = doc_df.drop(columns=[text_column])\n",
        "word_index = {}\n",
        "for index, row in doc_notxt_df.iterrows():\n",
        "  if use_named_entities:\n",
        "    for entsign in doc_index[index]:\n",
        "      ne = doc_index[index][entsign]\n",
        "      if entsign not in word_index:\n",
        "        word_index[entsign] = {'text': ne['NE-text'], 'type': ne['NE-type'], 'count-occurences-total':0, 'count-documents':0}\n",
        "      word_index[entsign]['count-occurences-total'] += ne['NE-count']\n",
        "      word_index[entsign]['count-documents'] += 1\n",
        "\n",
        "  else:\n",
        "    for wsign in doc_index[index]:\n",
        "      w = doc_index[index][wsign]\n",
        "      if wsign not in word_index:\n",
        "        word_index[wsign] = {'text': w['W-text'], 'count-occurences-total':0, 'count-documents':0}\n",
        "      word_index[wsign]['count-occurences-total'] += w['W-count']\n",
        "      word_index[wsign]['count-documents'] += 1\n",
        "\n",
        "word_df = pd.DataFrame(word_index.values())\n",
        "print(\"Done.\")\n",
        "print(\"Preview of the words list:\")\n",
        "word_df"
      ],
      "metadata": {
        "id": "ef9mq1f0ezxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregate pairs into a dataframe"
      ],
      "metadata": {
        "id": "xVeZt53vZifs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pair_list = []\n",
        "for index, row in doc_notxt_df.iterrows():\n",
        "  for entsign in doc_index[index]:\n",
        "    if word_index[entsign]['count-documents']>1:\n",
        "      new_row = {**row, **doc_index[index][entsign]}\n",
        "      pair_list.append(new_row)\n",
        "\n",
        "pair_df = pd.DataFrame(pair_list)\n",
        "print(\"Done.\")\n",
        "print(\"Preview of the pairs list:\")\n",
        "pair_df"
      ],
      "metadata": {
        "id": "2imc-2koZk9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save as CSV"
      ],
      "metadata": {
        "id": "x57hmk7DZlbq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiJ2zmLiUKYB"
      },
      "source": [
        "try:\n",
        "  pair_df.to_csv(output_file_pairs, index = False, encoding='utf-8')\n",
        "except IOError:\n",
        "  print(\"/!\\ Error while writing the pairs output file\")\n",
        "\n",
        "try:\n",
        "  word_df.to_csv(output_file_words, index = False, encoding='utf-8')\n",
        "except IOError:\n",
        "  print(\"/!\\ Error while writing the words output file\")\n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}