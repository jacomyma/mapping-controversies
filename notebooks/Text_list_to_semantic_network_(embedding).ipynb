{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVSVsEHncodV9q9En2dWyc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacomyma/mapping-controversies/blob/main/notebooks/Text_list_to_semantic_network_(embedding).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🌶️ Text list to semantic network (embedding)\n",
        "\n",
        "**Inputs:**\n",
        "* a list of texts as a CSV. The texts could be short (a word or sentence) or long (articles).\n",
        "\n",
        "**Outputs:**\n",
        "* a network of the texts with semantic links obtained from an embedding and dimensionality reduction (GraphML)."
      ],
      "metadata": {
        "id": "CdbYR1NucJDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use\n",
        "* Put your input file in the same folder as the notebook\n",
        "* Edit the **settings** below if needed\n",
        "* **Run all cells** (you can do that from the menu \"Runtime > Run all\").\n",
        "  * Note: if asked to restart the runtime, do it and re-run all cells\n",
        "* Scroll down to the bottom to **explore the visualization**.\n",
        "\n",
        "### Optional section\n",
        "* **RAG** (retrieval augmented generation):\n",
        "  * Jump to the last section of this notebook\n",
        "  * Edit the query\n",
        "  * Execute that cell and those below\n",
        "  * Copy-paste the generated prompt into the AI assistant of your choice"
      ],
      "metadata": {
        "id": "FB8Ndy0w3lKp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyhLt_B5bmBv"
      },
      "outputs": [],
      "source": [
        "# Input file\n",
        "input_file = 'texts.csv'\n",
        "\n",
        "# Which column contains the text?\n",
        "text_column = 'Text'\n",
        "\n",
        "# Limit the data points (via random sampling) to make a quick test; keep to 0 to analyze all\n",
        "max_data_points = 0\n",
        "\n",
        "# Output file\n",
        "output_file = \"semantic-network.graphml\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script starts here"
      ],
      "metadata": {
        "id": "3PfGAYKSfMe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep this to True as long as you re-run with a different input\n",
        "recompute_embeddings = True"
      ],
      "metadata": {
        "id": "73JFOLx26BVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries we need\n",
        "# (if already installed, it will auto-skip them, no worries)\n",
        "!pip install pandas chromadb bokeh umap-learn"
      ],
      "metadata": {
        "id": "NOqjerjDhLvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import json"
      ],
      "metadata": {
        "id": "KY8RSGkIftVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV\n",
        "df = pd.read_csv(input_file)\n",
        "df"
      ],
      "metadata": {
        "id": "XdYRW0uCUgB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle row order\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "cYs5bH1DnbJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncate data\n",
        "if max_data_points>0:\n",
        "  df = df.head(max_data_points)"
      ],
      "metadata": {
        "id": "P5It8h0UWaA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "# Initialize ChromaDB and create a collection\n",
        "client = chromadb.PersistentClient(path='embeddings')\n",
        "\n",
        "if recompute_embeddings:\n",
        "  # Delete the collection if it exists\n",
        "  try:\n",
        "      client.delete_collection(name=\"my_documents\")\n",
        "      print(\"Existing collection 'my_documents' deleted.\")\n",
        "  except:\n",
        "      pass  # Ignore if collection doesn't exist\n",
        "\n",
        "# Get or create the collection\n",
        "try:\n",
        "  collection = client.get_collection(name=\"my_documents\")\n",
        "except:\n",
        "  collection = client.create_collection(name=\"my_documents\")\n",
        "\n",
        "  # Load the desired Sentence Transformer model\n",
        "  embedding_model = SentenceTransformer(\"intfloat/multilingual-e5-large-instruct\")\n",
        "  # Note: Why this model?\n",
        "  #       Because it is multilingual and the best with Scandinavian languages at the moment.\n",
        "  #       Source: https://kennethenevoldsen.github.io/scandinavian-embedding-benchmark/\n",
        "\n",
        "  # Get the text content and metadata from the DataFrame\n",
        "  texts = df[text_column].tolist()\n",
        "  metadata = df.drop(columns=[text_column]).to_dict(orient=\"records\")\n",
        "\n",
        "  # Use DataFrame's row index as id\n",
        "  ids = df.index.astype(str).tolist()  # Convert index to strings for ChromaDB\n",
        "\n",
        "  batch_size = 256  # Adjust this based on your resources and testing\n",
        "\n",
        "  for i in range(0, len(texts), batch_size):\n",
        "    batch_texts = texts[i:i + batch_size]\n",
        "    batch_metadata = metadata[i:i + batch_size]\n",
        "    batch_ids = ids[i:i + batch_size]\n",
        "\n",
        "    batch_embeddings = embedding_model.encode(batch_texts)\n",
        "\n",
        "    # Convert embeddings to a list of lists if necessary (depending on SentenceTransformer version)\n",
        "    if not isinstance(batch_embeddings[0], list):\n",
        "      batch_embeddings = [emb.tolist() for emb in batch_embeddings]\n",
        "\n",
        "    collection.add(\n",
        "      documents=batch_texts,\n",
        "      metadatas=batch_metadata,\n",
        "      embeddings=batch_embeddings,\n",
        "      ids=batch_ids\n",
        "    )\n",
        "print('Done.')"
      ],
      "metadata": {
        "id": "t6mhBVBpgEYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = collection.get(include=['embeddings'])['embeddings']"
      ],
      "metadata": {
        "id": "34UITRrpw3hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "umap_n_neighbors = 8"
      ],
      "metadata": {
        "id": "cqvyvWaF85d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute UMAP for layout\n",
        "import umap\n",
        "\n",
        "if recompute_embeddings or 'umap_result' not in locals():\n",
        "  # Initialize UMAP with desired parameters\n",
        "  reducer = umap.UMAP(n_neighbors=umap_n_neighbors,\n",
        "                      n_components=2, # dimensions\n",
        "                      min_dist=0.15,\n",
        "                      metric='cosine',\n",
        "                      random_state=42)\n",
        "\n",
        "  # Apply UMAP to the embeddings\n",
        "  umap_result = reducer.fit_transform(embeddings)\n",
        "\n",
        "print(\"UMAP reduction complete.\")"
      ],
      "metadata": {
        "id": "bnpSIL5Sw5ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract KNN network à la UMAP\n",
        "import numpy as np\n",
        "from umap import UMAP\n",
        "from umap.umap_ import nearest_neighbors\n",
        "\n",
        "def get_umap_neighbors(data, n_neighbors=15, metric='cosine'):\n",
        "    # Convert data to numpy array if needed\n",
        "    X = np.asarray(data)\n",
        "\n",
        "    # Compute nearest neighbors using UMAP's internal function\n",
        "    # This returns the same neighbor graph that UMAP would use\n",
        "    knn_indices, knn_dists, _ = nearest_neighbors(\n",
        "        X,\n",
        "        n_neighbors=n_neighbors,\n",
        "        metric=metric,\n",
        "        metric_kwds={},\n",
        "        angular=False,\n",
        "        random_state=None\n",
        "    )\n",
        "\n",
        "    return knn_indices, knn_dists\n",
        "\n",
        "indices, distances = get_umap_neighbors(embeddings, n_neighbors=umap_n_neighbors)"
      ],
      "metadata": {
        "id": "8OLU01Tg8pDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Create a NetworkX graph\n",
        "graph = nx.Graph()\n",
        "\n",
        "# Add nodes (ids from the dataframe)\n",
        "for i in range(len(indices)):\n",
        "  id = f'n{i:06d}'\n",
        "  graph.add_node(id, x=umap_result[i, 0]*100, y=umap_result[i, 1]*100)\n",
        "  # Add all non-text columns of df as attributes\n",
        "  for col in df.columns:\n",
        "    if col != text_column:\n",
        "      graph.nodes[id][col] = df.iloc[i][col]\n",
        "\n",
        "# Add edges with weights (1/distance)\n",
        "for i in range(len(indices)):\n",
        "  for j in range(len(indices[i])):\n",
        "    neighbor_index = indices[i][j]\n",
        "    distance = distances[i][j]\n",
        "    if distance > 0:  # Avoid division by zero\n",
        "      weight = 1 / distance\n",
        "      id1 = f'n{i:06d}'\n",
        "      id2 = f'n{neighbor_index:06d}'\n",
        "      graph.add_edge(id1, id2, weight=weight)\n",
        "\n",
        "# Save the graph as a GEXF file\n",
        "nx.write_graphml(graph, output_file)"
      ],
      "metadata": {
        "id": "Zgu2YF2K9PdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bokeh\n",
        "import bokeh.plotting as bp\n",
        "from bokeh.io import output_notebook, show\n",
        "from bokeh.models import ColumnDataSource, HoverTool, CategoricalColorMapper"
      ],
      "metadata": {
        "id": "reGN4h3_i1Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ColumnDataSource for Bokeh\n",
        "source = ColumnDataSource(data=dict(\n",
        "    x=umap_result[:, 0],\n",
        "    y=umap_result[:, 1],\n",
        "    text=df[text_column],\n",
        "    id=df.index.astype(str),\n",
        "))"
      ],
      "metadata": {
        "id": "GMVHP51vi3rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory interface below\n",
        "It will appear once the cell is executed successfully."
      ],
      "metadata": {
        "id": "NPpSIfzfmY3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the plot to the notebook\n",
        "output_notebook()\n",
        "\n",
        "# Create the figure\n",
        "p = bp.figure(width=900, height=900,\n",
        "            title=f\"Semantic map (proximity indicates similarity)\",\n",
        "            tools=\"pan,wheel_zoom,box_zoom,reset,hover\", tooltips=[(\"Text\", \"@text\")], match_aspect=True)\n",
        "\n",
        "# Add scatter plot with color mapping and hover tool\n",
        "p.scatter('x', 'y', source=source, size=10)\n",
        "\n",
        "# Customize the plot (optional)\n",
        "p.xaxis.axis_label = \"UMAP Dimension 1\"\n",
        "p.yaxis.axis_label = \"UMAP Dimension 2\"\n",
        "\n",
        "# HTML tooltip\n",
        "hover = p.select(dict(type=HoverTool))\n",
        "hover.tooltips = \"\"\"\n",
        "    <div style=\"width: 300px; word-wrap: break-word;\">\n",
        "        <div>@text</div>\n",
        "        <div><em>&mdash;@actor, @source, @date</em></div>\n",
        "        <div>@issue</div>\n",
        "        <pre>ID: @id</pre>\n",
        "        <br>\n",
        "    </div>\n",
        "\"\"\"\n",
        "hover.mode = 'mouse'  # Enable HTML rendering\n",
        "\n",
        "# Show the plot\n",
        "show(p)"
      ],
      "metadata": {
        "id": "DqWdsy7mjEAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Augmented Generation (RAG)\n",
        "Edit the cell below (query), run it and the subsequent cells, then copy the last output into ChatGPT or another AI assistant."
      ],
      "metadata": {
        "id": "o-4kiDGibKRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EDIT THE QUERY BELOW\n",
        "# Then execute this cell and the following ones\n",
        "\n",
        "query = \"What is energy conversion?\"\n",
        "number_of_results = 10"
      ],
      "metadata": {
        "id": "HYfebfbvouYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed the query\n",
        "query_embedding = embedding_model.encode([query])\n",
        "\n",
        "# Perform a similarity search\n",
        "results = collection.query(\n",
        "    query_embeddings=query_embedding,\n",
        "    n_results=number_of_results # Number of chunks retrieved\n",
        ")\n",
        "\n",
        "# Extract the chunks from results\n",
        "retrievedChunks_txt = results['documents'][0]\n",
        "retrievedChunks_id = results['ids'][0]\n",
        "\n",
        "# Print them\n",
        "print(\"# Extracted sentences:\\n\")\n",
        "for chunk in retrievedChunks_txt:\n",
        "  print(\"\\n- \"+chunk)"
      ],
      "metadata": {
        "id": "qGl-9SIMbc97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot on the Bokeh figure\n",
        "\n",
        "# Create the figure\n",
        "p = bp.figure(width=900, height=900,\n",
        "            title=\"UMAP Visualization of text chunks (colored by source document)\",\n",
        "            tools=\"pan,wheel_zoom,box_zoom,reset,hover\", tooltips=[(\"ID\", \"@id\"), (\"Text\", \"@text\")], match_aspect=True)\n",
        "\n",
        "# Add scatter plot with color mapping and hover tool\n",
        "p.scatter('x', 'y', source=source, size=10)\n",
        "\n",
        "# Highlight closest chunks in red\n",
        "closest_indices = df[df.index.astype(str).isin(retrievedChunks_id)].index\n",
        "\n",
        "closest_source = ColumnDataSource(data=dict(\n",
        "    x=[umap_result[i, 0] for i in closest_indices],\n",
        "    y=[umap_result[i, 1] for i in closest_indices],\n",
        "))\n",
        "\n",
        "p.circle('x', 'y', source=closest_source, size=15, color=\"red\", legend_label=\"Closest Chunks\")\n",
        "\n",
        "# Customize the plot (optional)\n",
        "p.xaxis.axis_label = \"UMAP Dimension 1\"\n",
        "p.yaxis.axis_label = \"UMAP Dimension 2\"\n",
        "\n",
        "# HTML tooltip\n",
        "hover = p.select(dict(type=HoverTool))\n",
        "hover.tooltips = \"\"\"\n",
        "    <div style=\"width: 300px; word-wrap: break-word;\">\n",
        "        <div>@text</div>\n",
        "        <div><em>&mdash;@actor, @source, @date</em></div>\n",
        "        <div>@issue</div>\n",
        "        <pre>ID: @id</pre>\n",
        "        <br>\n",
        "    </div>\n",
        "\"\"\"\n",
        "hover.mode = 'mouse'  # Enable HTML rendering\n",
        "\n",
        "# Show the plot\n",
        "show(p)"
      ],
      "metadata": {
        "id": "3j112GVabtnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f'''You are an assistant answering queries informed by a specific context provided below in JSON.\n",
        "\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "\n",
        "QUERY\n",
        "```\n",
        "{query}\n",
        "```\n",
        "\n",
        "CONTEXT\n",
        "```json\n",
        "{json.dumps(retrievedChunks_txt)}\n",
        "```\n",
        "'''\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "vSqhKpGwdlrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_nqfDFFX46IB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}