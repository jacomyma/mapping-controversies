{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Words and documents with text to network",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacomyma/mapping-controversies/blob/main/notebooks/Words_and_documents_with_text_to_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🍇 Words and documents with text to network"
      ],
      "metadata": {
        "id": "JusW97SUGBgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inputs:**\n",
        "* a list of documents with their text content (CSV)\n",
        "* a list of words (CSV)\n",
        "\n",
        "**Outputs:**\n",
        "* a bipartite network of documents and words (GEXF)\n",
        "* a list of the documents, and how many of the words are in them, as columns (CSV)\n",
        "\n",
        "This script tells you which words are in which documents. Words can be expressions (e.g., named entities)."
      ],
      "metadata": {
        "id": "31PaqnfcGDWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use"
      ],
      "metadata": {
        "id": "cVzHKii6HvCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Put your input files in the same folder as the notebook\n",
        "1. Edit the settings if needed. CHECK THE COLUMN NAMES!\n",
        "1. Run all the cells\n",
        "1. Take the output file from the notebook folder"
      ],
      "metadata": {
        "id": "yiVPSZzJHz22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETTINGS"
      ],
      "metadata": {
        "id": "2aMAjhGDH2qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input file 1: documents\n",
        "input_file_documents = \"documents.csv\"\n",
        "# Which column contains the text?\n",
        "documents_text_column = \"Text\"\n",
        "# Which column contains the document name or ID?\n",
        "documents_id_column = \"Article\"\n",
        "\n",
        "# Input file 2: small list of words\n",
        "input_file_words = \"words.csv\"\n",
        "# Which column contains the words?\n",
        "words_text_column = \"text\"\n",
        "\n",
        "# Delete documents that contain none of the words?\n",
        "discard_unrelated_documents = True\n",
        "\n",
        "# Output file\n",
        "output_file_network = \"terms-document-network.gexf\""
      ],
      "metadata": {
        "id": "ODgqeaStH3kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCRIPT"
      ],
      "metadata": {
        "id": "9d_s_6zbH40x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import libraries\n",
        "This notebook draws on existing code.\n",
        "You can ignore the output."
      ],
      "metadata": {
        "id": "Swaxx6G4H8yS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU-bf9BtvSJN"
      },
      "source": [
        "# Install (if needed)\n",
        "!pip install pandas\n",
        "!pip install spacy\n",
        "!pip install networkx\n",
        "\n",
        "# Import\n",
        "import csv\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the input file 1 (documents)"
      ],
      "metadata": {
        "id": "d0FSMimJIZtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_df = pd.read_csv(input_file_documents, quotechar='\"', encoding='utf8', doublequote=True, quoting=csv.QUOTE_NONNUMERIC, dtype=object)\n",
        "print(\"Preview of the document list:\")\n",
        "doc_df"
      ],
      "metadata": {
        "id": "3OF7BDMiIbok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the input file 2 (words)"
      ],
      "metadata": {
        "id": "U9uSFdYiKoAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_df = pd.read_csv(input_file_words, quotechar='\"', encoding='utf8', doublequote=True, quoting=csv.QUOTE_NONNUMERIC, dtype=object)\n",
        "print(\"Preview of the word list:\")\n",
        "word_df"
      ],
      "metadata": {
        "id": "FXNwsPgVKlWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wrangle the data"
      ],
      "metadata": {
        "id": "eczPM9TFMO3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a set of the words\n",
        "words = set()\n",
        "for index, row in word_df.iterrows():\n",
        "  words.add(row[words_text_column])\n",
        "\n",
        "# Init data for output\n",
        "network_doc_set = set()\n",
        "network_word_set = set()\n",
        "network_edge_list = []\n",
        "\n",
        "# Search words in documents\n",
        "for index, row in doc_df.iterrows():\n",
        "  text = row[documents_text_column].lower()\n",
        "  count_per_word = {}\n",
        "  flag = False\n",
        "  for word in words:\n",
        "    count = text.count(word.lower())\n",
        "    count_per_word[word] = count\n",
        "    if count > 0:\n",
        "      flag = True\n",
        "\n",
        "  if flag or not discard_unrelated_documents:\n",
        "    doc_id = row[documents_id_column]\n",
        "    network_doc_set.add(doc_id)\n",
        "    for word in words:\n",
        "      count = count_per_word[word]\n",
        "      if count > 0:\n",
        "        network_word_set.add(word)\n",
        "        network_edge_list.append((doc_id,word,{\"count\":count}))\n",
        "\n"
      ],
      "metadata": {
        "id": "HA_qnH03MRBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make and save network"
      ],
      "metadata": {
        "id": "jdfseWV4SH8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the nodes\n",
        "nodes = []\n",
        "doc_df_no_text = doc_df.drop(columns=[documents_text_column]) \n",
        "for index, row in doc_df_no_text.iterrows():\n",
        "  if row[documents_id_column] in network_doc_set:\n",
        "    nodes.append((row[documents_id_column], {**row, 'label':row[documents_id_column], 'type':'document'}))\n",
        "\n",
        "for index, row in word_df.iterrows():\n",
        "  if row[words_text_column] in network_word_set:\n",
        "    nodes.append((row[words_text_column], {**row, 'label':row[words_text_column], 'type':'term'}))\n",
        "\n",
        "# Build edges\n",
        "edges = network_edge_list\n",
        "\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(nodes)\n",
        "G.add_edges_from(edges)\n",
        "nx.write_gexf(G, output_file_network)\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "P4pxNWmASJ6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make and save occurrences list"
      ],
      "metadata": {
        "id": "TPCB2WAYdMJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column = ['Id']\n",
        "\n",
        "for word in words:\n",
        "  column.append(word)\n",
        "word_occurences = pd.DataFrame(columns = column)\n",
        "\n",
        "# Build rows index\n",
        "rows = dict()\n",
        "for each in network_edge_list:\n",
        "  row = {'Id':each[0]}\n",
        "  for word in words:\n",
        "    row[word] = 0\n",
        "  rows[each[0]] = row\n",
        "\n",
        "for each in network_edge_list:\n",
        "  row = rows[each[0]]\n",
        "  for word in words:\n",
        "    if word == each[1]:\n",
        "      row.update({word:each[2]['count']})\n",
        "word_occurences = word_occurences.append(list(rows.values()), ignore_index=True)\n",
        "word_occurences.to_csv('word_occurences.csv', index=False)"
      ],
      "metadata": {
        "id": "KdEOc8yBQVLk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}