{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wikipedia articles to co-reference network",
      "provenance": [],
      "authorship_tag": "ABX9TyMAO0LBlDfo0EdKP32TvYwZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacomyma/mapping-controversies/blob/main/notebooks/Wikipedia_articles_to_co_reference_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wikipedia articles to co-reference network\n"
      ],
      "metadata": {
        "id": "zAwsuZhBpHiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input:** a list of Wikipedia articles (CSV).\n",
        "\n",
        "**Output:** a network of Wikipedia articles connected when they share one or references (GEXF).\n",
        "\n",
        "This scripts queries Wikipedia for each article of the input list. It retrieves the references for each article. Then it outputs a weighted, undirected network where the nodes are exactly the articles of the input list, and the edges are the references in common between these articles (if any).\n",
        "\n",
        "Note: some articles of the list might be redirections to other articles of the list, therefore the output may have less articles than the input."
      ],
      "metadata": {
        "id": "QSgmkIYBPabY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use"
      ],
      "metadata": {
        "id": "YjtFlahqPcd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Put your input file in the same folder as the notebook\n",
        "1. Edit the settings if needed\n",
        "1. Run all the cells\n",
        "1. Take the output file from the notebook folder"
      ],
      "metadata": {
        "id": "vUM-iw8qPdse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETTINGS"
      ],
      "metadata": {
        "id": "tWsy58lrpnbi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u63Bjnyjo00K"
      },
      "outputs": [],
      "source": [
        "# Input file\n",
        "input_file = \"wikipedia-articles.csv\"\n",
        "\n",
        "# Which column contains the article title?\n",
        "article_name_column = \"Article\"\n",
        "\n",
        "# Output file\n",
        "output_file = \"wikipedia-articles-coreference-network.GEXF\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCRIPT"
      ],
      "metadata": {
        "id": "sds4E-v2ptLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import libraries\n",
        "This notebook draws on existing code.\n",
        "You can ignore the output."
      ],
      "metadata": {
        "id": "B3UNkAgtpv8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install (if needed)\n",
        "!pip install wikipedia\n",
        "!pip install pandas\n",
        "!pip install networkx\n",
        "\n",
        "# Import\n",
        "import wikipedia\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import csv\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "6l1_v3w8pwhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the input file"
      ],
      "metadata": {
        "id": "GdQnYGGopyMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article_df = pd.read_csv(input_file, quotechar='\"', encoding='utf8', doublequote=True, quoting=csv.QUOTE_NONNUMERIC, dtype=object)\n",
        "print(\"Preview of the article list:\")\n",
        "article_df"
      ],
      "metadata": {
        "id": "KL4T4j2Bpz_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Harvest Wikipedia"
      ],
      "metadata": {
        "id": "WF1ziN1Zp2AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article_dict={}\n",
        "article_list=[]\n",
        "print(\"Harvesting references from \"+str(len(article_df.index))+\" wikipedia pages. This might take a while...\")\n",
        "count=1\n",
        "for title in article_df[article_name_column]:\n",
        "  if count % 50 == 0:\n",
        "    print(\"References harvested from \"+str(count)+\" pages out of \"+str(len(article_df.index))+\". Continuing...\")\n",
        "  count=count+1\n",
        "  try:\n",
        "    page = wikipedia.page(title,auto_suggest=False)\n",
        "  except wikipedia.exceptions.DisambiguationError:\n",
        "    print(\"Wikipedia thinks \"+title+\" is ambiguous (returns several candidate pages). Trying again with all capitalized letters\")\n",
        "    try:\n",
        "      page = wikipedia.page(title.capitalize(),auto_suggest=False)\n",
        "      print(\"Success! \"+title+\" is no longer ambiguous\")\n",
        "    except wikipedia.exceptions.DisambiguationError:\n",
        "      print(\"Wikipedia still thinks \"+title+\" is ambiguous (returns several candidate pages). Trying again with all lower letters\")\n",
        "      try:\n",
        "        page = wikipedia.page(title.lower(),auto_suggest=False)\n",
        "        print(\"Success! \"+title+\" is no longer ambiguous\")\n",
        "      except wikipedia.exceptions.DisambiguationError:\n",
        "        print(\"Wikipedia still thinks \"+title+\" is ambiguous (returns several candidate pages). Skipping page...\")\n",
        "        continue\n",
        "  except wikipedia.exceptions.PageError:\n",
        "    print(\"The page \"+title+\" could not be found. Skipping page...\")\n",
        "    continue\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "      \n",
        "  try:\n",
        "    refs = page.references\n",
        "    #  print(target_refs)\n",
        "    article_dict[title]={\"references\":refs}\n",
        "    article_list.append(title)\n",
        "\n",
        "  except KeyError:\n",
        "    print(\"Could not retrieve references for \"+title+\". Skipping page...\")\n",
        "    continue\n",
        "  \n",
        "print(\"Succesfully retrieved references from \"+str(len(article_dict))+\" out of \"+str(len(article_df.index))+\" wikipedia pages!\")\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "F14x2EB4p3yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build network"
      ],
      "metadata": {
        "id": "pj57eZybroFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building network...\")\n",
        "\n",
        "# Build the nodes, with attributes\n",
        "nodes = []\n",
        "for index, row in article_df.iterrows():\n",
        "  nodes.append((row[article_name_column], {**row, 'label':row[article_name_column]}))\n",
        "\n",
        "# Build edges (only between source nodes)\n",
        "edges = []\n",
        "for i,source in enumerate(article_list):\n",
        "  source_refs = article_dict[source][\"references\"]\n",
        "  if len(source_refs)>0:\n",
        "    for target in article_list[i+1:]:\n",
        "      if target==source:\n",
        "        continue\n",
        "      target_refs=article_dict[target][\"references\"]\n",
        "      if len(target_refs)>0:\n",
        "        overlap = len(set(source_refs).intersection(target_refs))\n",
        "        if overlap>0:\n",
        "          if len(source_refs) < len(target_refs):\n",
        "            norm_overlap_by_smallest = overlap / len(source_refs)\n",
        "          else:\n",
        "            norm_overlap_by_smallest = overlap / len(target_refs)\n",
        "          edge = (source,target,{'overlap':overlap,\n",
        "                                 'norm_overlap_by_smallest':norm_overlap_by_smallest,\n",
        "                                 'weight':norm_overlap_by_smallest})\n",
        "          edges.append(edge)\n",
        "\n",
        "print(\"Network has been generated. Saving...\")\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(nodes)\n",
        "G.add_edges_from(edges)\n",
        "nx.write_gexf(G, output_file)\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "xQVI-Vq-rXWP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}